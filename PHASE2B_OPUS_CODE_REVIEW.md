# Phase 2B Code Review Prompt for Opus

**Purpose:** Have Claude Opus conduct a comprehensive code quality review of all Phase 2B generated code (Sonnet-generated) to identify gaps, quality issues, and improvements.

---

## COPY & PASTE THIS TO OPUS

```
## CONTEXT: Phase 2B Code Review

You are Claude Opus, reviewing code generated by Claude Sonnet for Andy's Personal AI Assistant project.

**Your Task:** Conduct a comprehensive code review of ALL code generated in Phase 2B (Alembic migrations, service layer, and API integration). Identify what was done well, what could be better, and what might have been missed.

**Project Context:**
- Project: Personal AI Assistant v0.1 (single-user MVP)
- Language: Python 3.12
- Framework: FastAPI with SQLAlchemy ORM
- Database: SQLite with Alembic migrations
- Location: /Users/andy/Dev/personal-ai-assistant/
- Standards: STANDARDS_INTEGRATION.md (strict - functions <20 lines, 80%+ test coverage, comprehensive docstrings)

**Phase 2B Completion:**
- ✅ Spec 2B-1 Complete: Alembic migrations + test infrastructure
- ✅ Spec 2B-2 Complete: Service layer (CRUD, business logic)
- ✅ Spec 2B-3 Complete: API routes integrated with services

**Your Access:**
You have filesystem access to /Users/andy/Dev/personal-ai-assistant/ - read all code you need to review.

---

## REVIEW SCOPE

Review the following areas and provide detailed feedback:

### 1. CODE QUALITY

**Check:**
- Functions adhering to <20 line limit
- Max 3 parameters per function
- Single responsibility principle
- Type hints on all parameters and returns
- Docstrings with examples
- Comments explaining WHY, not WHAT
- No copy-pasted code
- DRY principle adherence

**Look for:**
- Functions that should be broken down
- Unclear variable/function names
- Magic numbers/strings that should be constants
- Overly complex logic that could be simplified

### 2. TEST COVERAGE

**Check:**
- 80%+ code coverage requirement met
- Unit tests for all models and services
- Integration tests for API endpoints
- Edge case coverage (boundary values, empty inputs, special characters)
- Error path testing (exceptions properly tested)
- Fixtures and factories used correctly

**Look for:**
- Untested code paths
- Happy-path-only tests (missing error cases)
- Shallow fixtures that don't cover variations
- Missing integration tests
- Test data that doesn't reflect reality

### 3. ERROR HANDLING

**Check:**
- All exceptions properly defined
- Clear, actionable error messages
- Service exceptions converted to HTTP responses
- Proper HTTP status codes (404 vs 400 vs 500)
- Ownership checks on all database operations (user_id verification)
- Database constraint violations handled gracefully

**Look for:**
- Silent failures or unhandled exceptions
- Generic error messages
- Missing validation
- Security issues (data leaks, unauthorized access)
- Race conditions or concurrent access problems

### 4. DATABASE & ORM

**Check:**
- SQLAlchemy queries are efficient
- Proper use of sessions (no dangling sessions)
- Foreign key relationships correct
- Cascade delete rules appropriate
- Indexes on frequently-queried fields
- No N+1 query problems

**Look for:**
- Inefficient queries that could use indexes
- Lazy loading issues
- Missing eager loading where needed
- Pagination implemented correctly
- Search/filter logic working as specified

### 5. API DESIGN

**Check:**
- Endpoints follow REST principles
- Consistent response format (APIResponse wrapper)
- Proper pagination/filtering/sorting
- Request validation with Pydantic
- Rate limiting in place
- Authentication/authorization correct

**Look for:**
- Endpoint logic that should be in service layer
- Thin routes (routes should not contain business logic)
- Inconsistent response formats
- Missing filters or search functionality
- Pagination edge cases

### 6. SPECIFICATION ADHERENCE

**Check:**
- All endpoints from Phase2A_Spec2_APISpecification.md implemented
- All service methods from Phase2B_Spec2_ServiceLayer.md implemented
- Request/response schemas match specifications
- Alembic migrations create correct schema
- Models match Phase2A_Spec1_DataModels.md

**Look for:**
- Missing endpoints or methods
- Specification drift (what was generated vs what was spec'd)
- Features added that weren't spec'd (scope creep)
- Features spec'd but not implemented

### 7. STANDARDS COMPLIANCE

**Check:**
- Naming conventions followed (snake_case, DescriptiveNoun, UPPERCASE constants)
- Module organization correct
- No circular imports
- Proper use of __init__.py files
- Configuration management

**Look for:**
- Inconsistent naming
- Poor module organization
- Hidden dependencies
- Configuration hard-coded that should be env vars

### 8. SECURITY

**Check:**
- API key validation working
- User data isolation (queries filter by user_id)
- Input validation prevents SQL injection
- No sensitive data in logs
- No secrets in repository

**Look for:**
- Authorization bypass possibilities
- Data exposure (one user seeing another's thoughts)
- Vulnerable database queries
- Sensitive information in error messages

### 9. DOCUMENTATION

**Check:**
- Docstrings on all public functions
- Examples in docstrings
- README updated for Phase 2B
- Alembic migrations documented
- Service layer usage documented

**Look for:**
- Missing docstrings
- Unclear docstrings
- No examples for complex functions
- Architecture decisions not documented (ADRs)

### 10. MISSING FUNCTIONALITY

**Check against specs:**
- Are all endpoints implemented?
- Are all service methods implemented?
- Is pagination/filtering/sorting complete?
- Is search functionality working?
- Are all validation rules enforced?
- Are all error cases handled?

**Look for:**
- TODO comments in code
- Stubbed methods
- Features mentioned in spec but not in code
- Edge cases not handled

---

## REVIEW OUTPUT FORMAT

For each issue found, provide:

**Issue Category:** [Code Quality / Tests / Error Handling / Database / API / Specs / Standards / Security / Documentation / Missing]

**Severity:** [Critical / High / Medium / Low]

**Location:** File path and function/class name

**Problem:** Clear description of what's wrong or missing

**Why It Matters:** Why this needs attention

**Recommendation:** Specific fix or improvement

**Code Example:** If applicable, show current code and suggested improvement

---

## FINAL SUMMARY

After reviewing all files, provide:

1. **Overall Quality Assessment** - Is this production-ready or what's needed?
2. **Top 5 Issues** - Most important things to fix
3. **Quick Wins** - Easy improvements that have high impact
4. **Deeper Review Needed** - Anything that needs more investigation
5. **Readiness for Phase 2C** - Can this move to Docker/deployment, or should issues be fixed first?

---

## INSTRUCTIONS

1. **Start by reading these files:**
   - Phase2B_Spec1_AlembicAndTesting.md (to understand requirements)
   - Phase2B_Spec2_ServiceLayer.md (to understand service requirements)
   - Phase2B_Spec3_APIIntegration.md (to understand API requirements)
   - STANDARDS_INTEGRATION.md (to understand quality standards)

2. **Then review the generated code:**
   - migrations/ directory (Alembic configuration)
   - tests/ directory (test coverage and quality)
   - src/services/ directory (business logic)
   - src/api/routes/ directory (API endpoints)
   - src/models/ directory (data models from Phase 2A)

3. **Be thorough but constructive** - The goal is to make good code great, not to criticize.

4. **Focus on what matters** - Security > Test coverage > Code style. Fix security issues first.

5. **Provide actionable feedback** - "This function is too long" is less useful than "This function should be split into X and Y. Here's how..."

Ready to review Phase 2B code. What would you like me to focus on most?
```

---

## How to Use

1. **Copy the code block above** (between the triple backticks)
2. **Open a NEW chat with Claude Opus**
3. **Paste the entire prompt**
4. Opus will have filesystem access and can review all code
5. Opus will provide detailed, constructive feedback

---

## What You'll Get Back

Opus will provide:

✅ **Issues Found** - Specific problems with location and fix recommendations
✅ **Test Coverage Analysis** - Whether 80%+ coverage requirement is met
✅ **Security Review** - Any authorization/data isolation issues
✅ **Standards Compliance** - Whether code follows STANDARDS_INTEGRATION.md
✅ **Missing Features** - Anything spec'd but not implemented
✅ **Quality Assessment** - Overall readiness and what needs fixing
✅ **Priority List** - What to fix first (critical, high, medium, low)

---

## Next Steps After Review

Based on Opus feedback:

1. **Critical/High Issues** - Fix immediately before Phase 2C
2. **Medium Issues** - Fix or document decision to defer
3. **Low Issues** - Document for future refactoring
4. **Quick Wins** - Can usually fix during review conversation
5. **Phase 2C Readiness** - Opus will tell you if ready to proceed

---

## Why Opus is Better for This

- **More thorough** - Finds subtle issues Sonnet might miss
- **Better judgment** - Distinguishes between "wrong" and "could be better"
- **Strategic thinking** - Suggests architectural improvements, not just fixes
- **Constructive** - Explains WHY something matters before suggesting fix
- **Experience** - Sees patterns from reviewing tons of code

This is exactly what Opus excels at - comprehensive code review and quality assessment.
