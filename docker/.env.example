# Personal AI Assistant - Environment Configuration
# Copy this file to .env and configure for your environment

# =============================================================================
# DATABASE CONFIGURATION
# =============================================================================
POSTGRES_USER=andy
POSTGRES_PASSWORD=your_secure_password_here
POSTGRES_DB=personal_ai_assistant
DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}

# =============================================================================
# AI BACKEND CONFIGURATION
# =============================================================================

# Available backends (comma-separated list)
# Options: openai, claude, ollama, mock
# - openai: OpenAI-compatible servers (llama.cpp, LM Studio, vLLM, etc.)
# - claude: Anthropic Claude API (requires ANTHROPIC_API_KEY)
# - ollama: Local Ollama server
# - mock: Mock backend for testing
AVAILABLE_BACKENDS=openai,claude,mock

# Primary backend to use first
# Should be one of the backends listed in AVAILABLE_BACKENDS
PRIMARY_BACKEND=openai

# Secondary/fallback backend (used if primary fails)
# Set to empty string to disable fallback
SECONDARY_BACKEND=claude

# Backend selection strategy
# Options: sequential, primary_only, parallel
# - sequential: Try primary, then secondary if primary fails
# - primary_only: Only use primary backend, fail if unavailable
# - parallel: Query both backends and use fastest response
BACKEND_SELECTION_STRATEGY=sequential

# -----------------------------------------------------------------------------
# OpenAI-Compatible Backend (llama.cpp, LM Studio, vLLM, etc.)
# -----------------------------------------------------------------------------
# Base URL for OpenAI-compatible server
# Examples:
#   - llama.cpp llama-server: http://localhost:8080/v1
#   - LM Studio: http://localhost:1234/v1
#   - vLLM: http://localhost:8000/v1
OPENAI_COMPATIBLE_BASE_URL=http://fuego:8080/v1

# Model name to use with OpenAI-compatible server
# Note: Some servers ignore this and use whatever model is loaded
# For llama.cpp, this can be any string (the loaded model is used)
OPENAI_COMPATIBLE_MODEL=llama-3.1-8b-instruct.gguf

# -----------------------------------------------------------------------------
# Claude Backend (Anthropic API)
# -----------------------------------------------------------------------------
# Anthropic API key (get from https://console.anthropic.com/)
# Required if "claude" is in AVAILABLE_BACKENDS
ANTHROPIC_API_KEY=sk-ant-api03-your-api-key-here

# -----------------------------------------------------------------------------
# Ollama Backend (Local Ollama Server)
# -----------------------------------------------------------------------------
# Ollama server URL
# Default: http://localhost:11434
OLLAMA_BASE_URL=http://192.168.7.187:11434

# Ollama model to use
# Examples: llama3.1:8b, gemma2:27b, mistral:latest
# List available models: ollama list
OLLAMA_MODEL=gemma2:27b

# =============================================================================
# WEBHOOK CONFIGURATION
# =============================================================================
# Secret for GitHub webhook validation
# Generate with: openssl rand -hex 32
WEBHOOK_SECRET=your_webhook_secret_here

# Deployment script path (called by webhook receiver)
DEPLOY_SCRIPT=/mnt/data2-pool/andy-ai/app/scripts/deploy.sh

# Deployment logs directory
LOG_DIR=/mnt/data2-pool/andy-ai/app/logs/deployments

# =============================================================================
# API CONFIGURATION
# =============================================================================
# API key for authenticating requests to the Personal AI Assistant API
# Generate with: python -c "import uuid; print(uuid.uuid4())"
API_KEY=550e8400-e29b-41d4-a716-446655440000

# API rate limiting (requests per minute)
RATE_LIMIT_PER_MINUTE=100

# =============================================================================
# LOGGING & DEBUGGING
# =============================================================================
# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Enable verbose backend metrics
ENABLE_METRICS=true

# =============================================================================
# SCHEDULER CONFIGURATION
# =============================================================================
# Background consciousness check schedule (cron format)
# Default: every 30 minutes during waking hours
# Format: minute hour day month day_of_week
# Examples:
#   - Every 30 minutes: */30 * * * *
#   - Every hour at :00: 0 * * * *
#   - 9am-5pm only: 0 9-17 * * *
CONSCIOUSNESS_CHECK_SCHEDULE=*/30 * * * *

# Enable/disable automatic consciousness checks
ENABLE_CONSCIOUSNESS_CHECKS=true

# =============================================================================
# DEPLOYMENT NOTES
# =============================================================================
# 
# IMPORTANT: After copying this file to .env:
# 1. Update all "your_*_here" placeholders with real values
# 2. Generate secure values for API_KEY and WEBHOOK_SECRET
# 3. Configure at least one AI backend (openai, claude, or ollama)
# 4. Never commit .env to version control (it contains secrets!)
# 
# For production deployment on TrueNAS (moria):
# - Copy this to /mnt/data2-pool/andy-ai/app/docker/.env
# - Configure OPENAI_COMPATIBLE_BASE_URL to point to your local AI server
#   Example for llama.cpp on fuego: http://fuego:8080/v1
# - Set up GitHub webhook with matching WEBHOOK_SECRET
# 
# For local development:
# - Copy this to docker/.env in your local repo
# - Use localhost URLs for all services
# - Can use mock backend for testing without real AI APIs
#
# COMMON BACKEND CONFIGURATIONS:
# 
# llama.cpp (llama-server):
#   OPENAI_COMPATIBLE_BASE_URL=http://localhost:8080/v1
#   OPENAI_COMPATIBLE_MODEL=llama-3.1-8b-instruct.gguf
#
# LM Studio:
#   OPENAI_COMPATIBLE_BASE_URL=http://localhost:1234/v1
#   OPENAI_COMPATIBLE_MODEL=local-model
#
# vLLM:
#   OPENAI_COMPATIBLE_BASE_URL=http://localhost:8000/v1
#   OPENAI_COMPATIBLE_MODEL=meta-llama/Llama-3.1-8B-Instruct
#
# Ollama (alternative to OpenAI-compatible):
#   AVAILABLE_BACKENDS=ollama,claude
#   PRIMARY_BACKEND=ollama
#   OLLAMA_BASE_URL=http://localhost:11434
#   OLLAMA_MODEL=llama3.1:8b
#
